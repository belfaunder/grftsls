% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tsls_forest.R
\name{tsls_forest}
\alias{tsls_forest}
\title{Two-Stage-Least_Squares forest}
\usage{
tsls_forest(
  X,
  Y,
  W,
  Z,
  Y.hat,
  W.hat,
  Z.hat,
  sample.fraction = NULL,
  mtry = NULL,
  num.trees = 2000,
  min.node.size = NULL,
  honesty = TRUE,
  honesty.fraction = 0.5,
  ci.group.size = 2,
  alpha = NULL,
  imbalance.penalty = NULL,
  clusters = NULL,
  samples.per.cluster = NULL,
  tune.parameters = TRUE,
  num.fit.trees = 10,
  num.fit.reps = 100,
  num.optimize.reps = 1000,
  compute.oob.predictions = TRUE,
  num.threads = NULL,
  seed = NULL
)
}
\arguments{
\item{X}{The covariates used in the instrumental regression.}

\item{Y}{The outcome.}

\item{W}{The treatment assignment}

\item{Z}{The instruments}

\item{Y.hat}{Estimates of the expected responses E[Y | Xi], marginalizing
over treatment. If Y.hat = NULL, these are estimated using
a separate regression forest. Default is 0}

\item{W.hat}{Estimates of the treatment propensities E[W | Xi]. If W.hat = NULL,
these are estimated using a separate regression forest. Default is 0}

\item{Z.hat}{Estimates of the expected instruments E[Z | Xi]. If Z.hat = NULL,
these are estimated using a separate regression forest. Default is 0}

\item{sample.fraction}{Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.}

\item{mtry}{Number of variables tried for each split. Default is NULL.}

\item{num.trees}{Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.}

\item{min.node.size}{A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is NULL.}

\item{honesty}{Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, and recommendations for
parameter tuning, see the grf algorithm reference.}

\item{honesty.fraction}{The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).}

\item{ci.group.size}{The forst will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 2.}

\item{alpha}{A tuning parameter that controls the maximum imbalance of a split. Default is NULL.}

\item{imbalance.penalty}{A tuning parameter that controls how harshly imbalanced splits are penalized. Default is NULL.}

\item{clusters}{Not used}

\item{tune.parameters}{If TRUE, all paramters that are NULL are tuned.}

\item{compute.oob.predictions}{Whether OOB predictions on training set should be precomputed. Default is TRUE.}

\item{num.threads}{Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.}

\item{seed}{The seed of the C++ random number generator.}

\item{sample.weights}{Not used}

\item{tune.num.trees}{The number of trees in each 'mini forest' used to fit the tuning model.}

\item{tune.num.reps}{The number of forests used to fit the tuning model.}

\item{tune.num.draws}{The number of random parameter values considered when using the model
to select the optimal parameters.}
}
\value{
A trained two-stage least squares forest object.
}
\description{
Extends previous codes of the grf-package to a two-stage least squares forest
}
